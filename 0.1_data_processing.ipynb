{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for YOLONAS based detection models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook contains the code necessary for: \n",
    "* Rescaling the images to 1080 x 1080 pixels without distortion by completing the longest side with zeros\n",
    "* Computing the Region Of Interest coordinates for the new size\n",
    "* Generating a training dataset with labels and normalized coordinates in .txt format [label centerX centerY width height].\n",
    "\n",
    "The abnormality classes adressed in this experiment are: 1 Architectural distortion, 2 Mass and 3 Calcification\n",
    "\n",
    "The public datasets used are: \n",
    "\n",
    "* MIAS\n",
    "* CBIS-DDSM\n",
    "* CDD-CESM\n",
    "* INbreast\n",
    "* BMCD\n",
    "* VinDr\n",
    "\n",
    "To which I'll provide a link to their respective documentation within the README.md. I will also provide in this repository the .xlsx file containing the coordinates to every ROI. Take into consideration that the images from the MIAS database have been rotated, squared and transformed into DICOM so the coodinates here displayed will not fit the original PGM images. \n",
    "\n",
    "Don't forget to modify the image paths in the 'AbsPath' column according to your own storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "from processing import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "this_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinates file \n",
    "\n",
    "This code computes the necessary data to obtain both COCO and YOLO format labels. Larger dataframes will take a long time to process. Progress can be tracked with the tdqm module.\n",
    "This cell has already been run, so you can save time by using the .csv as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '..\\\\DetectorDatasetMG\\\\Templatedetfile.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6678/6678 [57:31<00:00,  1.93it/s]  \n"
     ]
    }
   ],
   "source": [
    "prev_row = None\n",
    "current_num = 1\n",
    "current_idx = 1\n",
    "\n",
    "def update_row(row):\n",
    "    global prev_row, current_num, current_idx\n",
    "    row, prev_row, current_num, current_idx = assign_image_numbers(row, prev_row, current_num, current_idx)\n",
    "    row = assign_labels_and_names(row)\n",
    "    row = resized_normalized_coordinates(row)\n",
    "    return row\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "tqdm.pandas() # Initialize tqdm for progress bar\n",
    "\n",
    "\n",
    "df_progress = df.progress_apply(update_row, axis=1) # Apply the transformation with tqdm for progress tracking\n",
    "df_progress.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to get a different dataset split [train, test, valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_dl_folder (csv_file, ratio):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    #Just to be sure...\n",
    "    assert len(ratio) == 3, \"Ratio must be a tuple of three elements\"\n",
    "    assert sum(ratio) <= 1, \"Sum of ratio must be less than or equal to 1\"\n",
    "    \n",
    "    categories = ['train', 'valid', 'test']\n",
    "    \n",
    "    df['set'] = np.random.choice(categories, size=len(df), p=ratio)\n",
    "    \n",
    "    df.to_csv(csv_file, index =False)\n",
    "\n",
    "assign_dl_folder(csv_file, ratio=(0.7, 0.2, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Excel file contains all the needed information to generate the dataset. Please review the comments on utils.py and processing.py for further information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing\n",
    "#### WARNING: This algorithm is not yet GPU accelerated and takes a long time to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(row, root):\n",
    "    if row['AbsPath'].endswith(('.dcm', '.dicom', '.DCM')):\n",
    "        image = dicom_preprocessing(row['AbsPath'], row['pixy'], row['pixx'])\n",
    "    else:\n",
    "        image = cv2.imread(row['AbsPath'])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image = reduce_poisson_noise(image)\n",
    "    squared = add_zeros_for_square(image, row['pixy'], row['pixx'])\n",
    "    det_size = (1080, 1080)\n",
    "    resized_squared = cv2.resize(squared, det_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Build storing path\n",
    "    image_folder = os.path.join(root, row['set'], 'images')\n",
    "    os.makedirs(image_folder, exist_ok=True)\n",
    "    image_name = f\"{row['ImageName']}.png\"\n",
    "    image_path = os.path.join(image_folder, image_name)\n",
    "    cv2.imwrite(image_path, resized_squared)\n",
    "\n",
    "    # Write label information to text file\n",
    "    label_folder = os.path.join(root, row['set'], 'labels')\n",
    "    os.makedirs(label_folder, exist_ok=True)\n",
    "    label_file_path = os.path.join(label_folder, f\"{row['ImageName']}_{row['Index']}.txt\")\n",
    "    with open(label_file_path, 'w') as f:\n",
    "        f.write(f\"{row['Label']} {row['cx']} {row['cy']} {row['nw']} {row['nh']}\")\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \" \"\n",
    "df = pd.read_csv(csv_file)\n",
    "tqdm.pandas() \n",
    "df_process_progress = df.progress_apply(lambda row: data_processing(row, root), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1% reached at 5 minutes "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
