{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Class image dataset of mammography with abnormalities for YOLONAS based detection models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook contains the code necessary for: \n",
    "* Rescaling the images to 1080 x 1080 pixels without distortion by completing the longest side with zeros\n",
    "* Computing the Region Of Interest coordinates for the new size\n",
    "* Generating a training dataset with labels and normalized coordinates in .txt format [label centerx centery width height].\n",
    "\n",
    "The abnormality classes adressed in this experiment are: 1 Architectural distortion, 2 Mass and 3 Calcification\n",
    "\n",
    "The public datasets used are: \n",
    "\n",
    "* MIAS\n",
    "* CBIS-DDSM\n",
    "* CDD-CESM\n",
    "* INbreast\n",
    "* BMCD\n",
    "* VinDr\n",
    "\n",
    "To which I'll provide a link to their respective documentation within the README.md. I will also provide in this repository the .xlsx file containing the coordinates to every ROI. There are 5191 images and 6678 abnormalities in total. \n",
    "\n",
    "Don't forget to modify the image paths in the 'AbsPath' column according to your own storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "from processing import *\n",
    "import torch\n",
    "#import cudf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "this_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinates file \n",
    "\n",
    "This code computes the necessary data to obtain both COCO and YOLO format labels. Larger dataframes will take a long time to process. Progress can be tracked with the tdqm module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6678/6678 [57:31<00:00,  1.93it/s]  \n"
     ]
    }
   ],
   "source": [
    "csv_file = '..\\\\DetectorDatasetMG\\\\Templatedetfile.csv'\n",
    "\n",
    "prev_row = None\n",
    "current_num = 1\n",
    "current_idx = 1\n",
    "\n",
    "def update_row(row):\n",
    "    global prev_row, current_num, current_idx\n",
    "    row, prev_row, current_num, current_idx = assign_image_numbers(row, prev_row, current_num, current_idx)\n",
    "    row = assign_labels_and_names(row)\n",
    "    row = resized_normalized_coordinates(row)\n",
    "    return row\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "tqdm.pandas() # Initialize tqdm for progress bar\n",
    "\n",
    "\n",
    "df_progress = df.progress_apply(update_row, axis=1) # Apply the transformation with tqdm for progress tracking\n",
    "df_progress.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the final dataset is randomly split into the train, valid and test categories\n",
    "def assign_dl_folder (csv_file, ratio):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    #Just to be sure...\n",
    "    assert len(ratio) == 3, \"Ratio must be a tuple of three elements\"\n",
    "    assert sum(ratio) <= 1, \"Sum of ratio must be less than or equal to 1\"\n",
    "    \n",
    "    categories = ['train', 'valid', 'test']\n",
    "    \n",
    "    df['set'] = np.random.choice(categories, size=len(df), p=ratio)\n",
    "    \n",
    "    df.to_csv(csv_file, index =False)\n",
    "\n",
    "assign_dl_folder(csv_file, ratio=(0.7, 0.2, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Excel file contains all the needed information to generate the dataset. Please review the comments on utils.py and processing.py for further information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing\n",
    "#### Run this version below if you don't have a GPU. It will take very long to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a dataset of normalized and denoised mammography images for better model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_for_detector (file, root):\n",
    "    df = pd.read_excel(file)\n",
    "    \n",
    "    # Define data\n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        if row['AbsPath'].endswith('.dcm') or row['AbsPath'].endswith ('.dicom') or row['AbsPath'].endswith('.DCM'): \n",
    "            image = dicom_preprocessing(row['AbsPath'], row['pixy'], row['pixx'])\n",
    "            \n",
    "        else:\n",
    "            image = cv2.imread(row['AbsPath'])\n",
    "            image = cv2.normalize(image.astype(float), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        \n",
    "        image = reduce_poisson_noise(image)\n",
    "        \n",
    "        squared = add_zeros_for_square(image, row['pixy'], row['pixx'])\n",
    "        det_size = (1080, 1080)\n",
    "        resized_squared = cv2.resize(squared, det_size, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Build storing path\n",
    "        dest_folder = os.path.join(root, row['set'],'images')\n",
    "        if not os.path.exists(dest_folder):\n",
    "            os.makedirs(dest_folder)\n",
    "            \n",
    "        # Save the images\n",
    "        image_name = f\"{row['ImageName']}_{row['Index']}.png\"\n",
    "        image_path = os.path.join(dest_folder, image_name)\n",
    "        cv2.imwrite(image_path, resized_squared)\n",
    "        \n",
    "        # Write label information to text file\n",
    "        label_folder = os.path.join(root, row['set'], 'labels')\n",
    "        if not os.path.exists(label_folder):\n",
    "            os.makedirs(label_folder)\n",
    "            \n",
    "        \n",
    "        label_file_path = os.path.join(label_folder, f\"{row['ImageName']}_{row['Index']}.txt\")\n",
    "        with open(label_file_path, 'w') as f:\n",
    "            f.write(f\"{row['Label']} {row['cx']} {row['cy']} {row['nw']} {row['nh']}\")\n",
    "            \n",
    "        print (row['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_detector(csv_file, root=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this version below if you have a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is still under construction\n",
    "def dataset_for_detector(file, root):\n",
    "    df = pd.read_excel(file)\n",
    "    df_cudf = cudf.DataFrame.from_pandas(df)\n",
    "\n",
    "    for i, row in df_cudf.iterrows():\n",
    "        abs_path = row['AbsPath'].to_pandas()\n",
    "        pixy = row['pixy']\n",
    "        pixx = row['pixx']\n",
    "\n",
    "        if abs_path.endswith('.dcm') or abs_path.endswith('.dicom') or abs_path.endswith('.DCM'): \n",
    "            image = dicom_preprocessing(abs_path, pixy, pixx)\n",
    "        else:\n",
    "            image = cv2.imread(abs_path)\n",
    "            image = cv2.normalize(image.astype(float), None, 0, 255, cv2.NORM_MINMAX).astype(cp.uint8)\n",
    "\n",
    "        # GPU-accelerated reduce_poisson_noise and add_zeros_for_square functions if possible\n",
    "        image = reduce_poisson_noise(image)  # Ensure this function is optimized for GPU\n",
    "        squared = add_zeros_for_square(image, pixy, pixx)  # Ensure this function is optimized for GPU\n",
    "\n",
    "        det_size = (1080, 1080)\n",
    "        resized_squared = cv2.resize(cp.asnumpy(squared), det_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # Build storing path\n",
    "        dest_folder = os.path.join(root, row['set'].to_pandas(),'images')\n",
    "        if not os.path.exists(dest_folder):\n",
    "            os.makedirs(dest_folder)\n",
    "            \n",
    "        # Save the images\n",
    "        image_name = f\"{row['ImageName'].to_pandas()}_{row['Index']}.png\"\n",
    "        image_path = os.path.join(dest_folder, image_name)\n",
    "        cv2.imwrite(image_path, resized_squared)\n",
    "        \n",
    "        # Write label information to text file\n",
    "        label_folder = os.path.join(root, row['set'].to_pandas(), 'labels')\n",
    "        if not os.path.exists(label_folder):\n",
    "            os.makedirs(label_folder)\n",
    "            \n",
    "        label_file_path = os.path.join(label_folder, f\"{row['ImageName'].to_pandas()}_{row['Index']}.txt\")\n",
    "        with open(label_file_path, 'w') as f:\n",
    "            f.write(f\"{row['Label']} {row['cx']} {row['cy']} {row['nw']} {row['nh']}\")\n",
    "            \n",
    "        print(row['id'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
